# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

# 1. Load the dataset and display the first 5 rows
# Replace with the correct path to your autompg.csv file
data = pd.read_csv('C:/Users/mkom8/Downloads/autompg.csv')  # <-- Change this to your file path
print("First 5 rows of the dataset:")
print(data.head())

# 2. Replace '?' with NaN
data = data.replace('?', np.nan)

# 3. Convert applicable columns to numeric (where it makes sense)
# Assuming there are columns that should be numeric but contain strings like '?'
data['horsepower'] = pd.to_numeric(data['horsepower'], errors='coerce')

# 4. Check for missing values and replace them with the median for numeric columns
numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns
data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())
print("\nMissing values after replacing '?' and handling with median:")
print(data.isnull().sum())

# 5. Check for duplicate rows and remove them
print(f'\nNumber of duplicate rows: {data.duplicated().sum()}')
data = data.drop_duplicates()
print(f'Number of rows after removing duplicates: {data.shape[0]}')

# 6. Create a correlation matrix and visualize using a heatmap
print("\nCorrelation matrix of the dataset:")
numeric_data = data.select_dtypes(include=[np.number])  # Select only numeric columns
corr_matrix = numeric_data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

# 7. Add a calculated field (weight in kilograms from pounds)
data['weight_kg'] = (data['weight'] * 0.453592).astype(int)
print("\nFirst 5 rows showing 'weight' and 'weight_kg' columns:")
print(data[['weight', 'weight_kg']].head())

# 8. Normalize features using Z-score method
scaler = StandardScaler()
numeric_data = data.select_dtypes(include=[float, int])
normalized_data = pd.DataFrame(scaler.fit_transform(numeric_data), columns=numeric_data.columns)
print("\nFirst 5 rows of normalized data:")
print(normalized_data.head())

# 9. Apply PCA to reduce dimensions to 2 components
# Ensure there are no missing values left in the normalized data
if normalized_data.isnull().sum().sum() == 0:  # Check if there are no NaN values
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(normalized_data)
    
    # 10. Store PCA result in a DataFrame
    pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
    print("\nFirst 5 rows of PCA result:")
    print(pca_df.head())
    
    # 11. Plot a scatter plot for PCA result
    plt.figure(figsize=(8,6))
    plt.scatter(pca_df['PC1'], pca_df['PC2'])
    plt.title('PCA Result (2 components)')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()
else:
    print("There are still missing values in the data, handle them before PCA.")
